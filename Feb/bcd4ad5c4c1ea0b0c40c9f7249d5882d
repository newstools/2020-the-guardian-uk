In 2012, a small group of young men, former supporters of the libertarian Republican congressman Ron Paul, started a blog called The Right Stuff. They soon began calling themselves “post-libertarians,” although they weren’t yet sure what would come next. By 2014, they’d started to self-identify as “alt-right”. They developed a countercultural tone – arch, antic, floridly offensive – that appealed to a growing cohort of disaffected young men, searching for meaning and addicted to the internet. These young men often referred to The Right Stuff, approvingly, as a key part of a “libertarian-to-far-right pipeline”, a path by which “normies” could advance, through a series of epiphanies, toward “full radicalisation”. As with everything the alt-right said, it was hard to tell whether they were joking, half-joking or not joking at all. The Right Stuff ’s founders came up with talking points – narratives, they called them – that their followers then disseminated through various social networks. On Facebook, they posted Photoshopped images, or parody songs, or “countersignal memes” – sardonic line drawings designed to spark just enough cognitive dissonance to shock normies out of their complacency. On Twitter, the alt-right trolled and harassed mainstream journalists, hoping to work the referees of the national discourse while capturing the attention of the wider public. On Reddit and 4chan and 8chan, where the content moderation was so lax as to be almost non-existent, the memes were more overtly vile. Many alt-right trolls started calling themselves “fashy”, or “fash-ist”. They referred to all liberals and traditional conservatives as communists, or “degenerates”; they posted pro-Pinochet propaganda; they baited normies into arguments by insisting that “Hitler did nothing wrong”. When I first saw luridly ugly memes like this, in 2014 and 2015, I wasn’t sure how seriously to take them. Everyone knows the most basic rule of the internet: don’t feed the trolls, and don’t take tricksters at their word. The trolls of the alt-right called themselves provocateurs, or shitposters, or edgelords. And what could be edgier than joking about Hitler? For a little while, I was able to avoid reaching the conclusion that would soon become obvious: maybe they meant what they said. I spent about three years immersing myself in two worlds: the world of these edgelords – meta-media insurgents who arrayed themselves in opposition to almost all forms of traditional gatekeeping – and the world of the new gatekeepers of Silicon Valley, who, whether intentionally or not, afforded the gatecrashers their unprecedented power. “The left won by seizing control of media and academia,” a blogger on The Right Stuff, using the pseudonym Meow Blitz, wrote in 2015. “With the internet, they lost control of the narrative.” By “the left”, he meant the whole standard range of American culture and politics – everyone who preferred democracy to autocracy, everyone who resisted the alt-right’s vision of a white American ethnostate. For decades, Meow Blitz argued, this pluralistic worldview – the mainstream worldview – had gone effectively unchallenged, but now, by promoting their agenda on social media, he and his fellow propagandists could push the US in a more fascist-friendly direction. “Isis became the most powerful terrorist group in the world because of flashy internet videos,” he wrote. “If you’re alive in the year 2015 and you don’t understand the power of the interwebz you’re an idiot.” To the post’s intended audience, this was supposed to be invigorating. To me, it was more like a faint whiff of sulphur that may or may not turn out to be a gas leak. The post was called “Right Wing Trolls Can Win”. Would the neofascists win? I had a hard time imagining it. Could they win? That was a different question. “The culture war is being fought daily from your smartphone,” the post continued. On this one point, at least, I had to agree with Meow Blitz. To change how we talk is to change who we are. During the long 2016 presidential campaign, Donald Trump seemed to draw on pools of dark energy not previously observed within the universe of the American electorate. The mainstream media used the catchall term alt-right, which appealed to newspaper editors and TV-news producers who hoped to connote frisson and novelty without passing explicit judgment. Instead of denouncing the alt-right, reporters often described it as “divisive” or “racially charged”. They tried to present both sides neutrally, as journalistic convention seemed to require. The definition of alt-right continued to expand. By the summer of 2016, it was such a big tent that it included any conservative or reactionary who was active online and too belligerently anti-establishment to feel at home in the Republican party – a category that included the Republican nominee for president. This was an oddly broad definition for what was supposed to be a fringe movement, and yet no one seemed eager to clear up the semantic confusion. The Clinton campaign played up the alt-right’s size and influence, while the alt-right was all too glad to be perceived as vast and menacing. There was no way to measure precisely how many Americans were alt-right, and there never would be. Estimates ranged from a few hundred to a few million. Still, what mattered was not the movement’s headcount, but its collective impact on the national vocabulary. “We’re the platform for the alt-right,” Steve Bannon said in July 2016, when he was running the pro-Trump web tabloid Breitbart. Later that year, after leading the Trump campaign to victory and being tapped to serve as chief White House strategist, Bannon claimed that he’d only meant to align himself with an insurgent brand of civic nationalism, not with ethno-nationalism. Yet a core within the movement still insisted on a narrower definition of alt-right, one based on explicit antisemitism and white supremacy. This core had always existed; no one who was versed in the far-right blogosphere could have missed it. Mainstream journalists, or at least the ones who were paying attention, were daunted by the fiscal precarity of their industry, the plummeting cultural authority of their institutions, and the unpredictable dynamics of social media outrage. The more these threats loomed, the more journalists clung to one of the few professional axioms that still seemed beyond dispute: in all matters of political opinion, a reporter should strive to remain neutral. This is true enough, for certain kinds of journalists, when applied to certain prosaic debates about tariffs and treaties. When it comes to core matters of principle, though, it’s not always possible to be both even-handed and honest. The plain fact was that the alt-right was a racist movement full of creeps and liars. If a newspaper’s house style didn’t allow its reporters to say so, at least by implication, then the house style was preventing its reporters from telling the truth. Neutrality has never been a universal good, even in the simplest of times. In unusual times – say, when the press has been drafted, without its consent or comprehension, into a dirty culture war – neutrality might not always be possible. Some questions aren’t really questions at all. Should Muslim Americans be treated as real Americans? Should women be welcome in the workplace? To treat these as legitimate topics of debate is to be not neutral, but complicit. Sometimes, even for a journalist, there is no such thing as not picking a side. In April 2014, looking for new story ideas, I attended a tech conference in a stylish hotel in Lower Manhattan. The conference was called F.ounders, a word that no one, including the founders of F.ounders, could decide how to pronounce. Half of us stammered over the stray full stop. The other half ignored it. It stood for nothing, apparently, except for the general concept of innovation. At this point, Google owned almost 40% of the online advertising market, and Facebook owned another 10%. Some analysts were already warning that they might comprise a duopoly. Both companies’ business models, especially Facebook’s, were built around microtargeting. Filter bubbles, in other words, were not a temporary bug but a central feature of social media. It was hard to see how the latter could flourish without the former. If filter bubbles were bad for democracy, then, were Google and Facebook also bad for democracy? It was a fair question, almost an obvious one, and yet the cultural vocabulary of the time did not allow most people to hold it in their heads for long. The Arab spring of 2011 had been organised, in part, via social media, and was often called the Twitter revolution. Mark Zuckerberg had been named Time’s person of the year in 2010; in the hagiographic cover photo, his eyes were oceanic and farseeing, dreaming up ingenious new ways to forge human bonds. If some movies and books portrayed him as shifty, even a bit ruthless, it was still possible to imagine that ruthlessness, in the tradition of Thomas Edison or Steve Jobs, was merely the cost of doing business. Zuckerberg’s motto, “Move fast and break things”, was generally treated as a sign of youthful insouciance, not of galling rapacity. Facebook’s users – more than a billion of them – seemed happy. Its investors were delighted. If social media wasn’t a good product, then why was it so successful? At the time, it was still considered divisive (at swanky New York tech conferences, anyway) to wonder whether the be-hoodied young innovators of Silicon Valley might turn out to be robber barons. It was far more socially acceptable to extol the gleaming vehicle of technology – to gaze in amoral awe at its speed and vigour – than to ask precisely where it was headed, or whether it might one day hurtle off a cliff. Such questions had come to seem fusty and antidemocratic; people who spent too much time worrying about them were often dismissed as cranks or luddites. To a techno-optimist, there was only one way the vehicle could possibly be going: forward. When it was founded in 2004, Facebook billed itself as “an online directory that connects people through social networks at colleges”. Within a few years, this self-description had morphed into a far more grandiose mission statement: “Facebook gives people the power to share and make the world more open and connected.” Mark Zuckerberg was careful not to call himself a gatekeeper. On the contrary, he portrayed himself as a Robin Hood figure, snatching power from the gatekeepers and redistributing it to the people, who could presumably be trusted to do the right thing. The traditional gatekeeper media that held sway in the US in the middle of the 20th century was, inarguably, a deeply flawed system. The nation’s most prominent journalists, from celebrity newscasters to unheralded assignment editors, were, by and large, upper-middle-class white men in grey suits. Many were blinkered coastal elites, either too circumspect or too myopic to risk departing meaningfully from the socially acceptable narrative, even when elements of that narrative were misleading or flat-out false. But what if the fourth estate turned out to be, like democracy, the worst system except for all the others? If history was an arc bending inexorably toward justice, then there was no need to worry about any of this – technological disruption could only lead the world more efficiently in the right direction. If history was contingent, however, then removing the gatekeepers, without any clear notion of what might replace them, could throw the whole information ecosystem into chaos. At a F.ounders dinner, the seating algorithm placed me next to Emerson Spartz, a 27-year-old with the saucer eyes and cuspidate chin of a cartoon fawn. His bio described him as a “middle-school dropout”, a “New York Times bestselling author” and the founder and CEO of Spartz Inc, based in Chicago. I asked what his company made, or did, or was. “I’m passionate about virality,” he responded. I must have looked confused, because he said: “Let me bring that down from the 30,000-foot level.” The appetiser course had not yet arrived. He checked the time on his cell phone, then cleared his throat. “Every day, when I was a kid, my parents made me read four short biographies of very successful people,” he said. “I decided that I wanted to change the world, and I wanted to do it on a massive scale.” This was the beginning of what I would come to recognize as his standard pitch for Spartz, both the person and the company. Although he had an audience of one, he spoke in a distant and deliberate tone, using studied pauses and facial expressions, as if I were a conference hall or a camera lens. “I looked at patterns,” he said. “I realised that if you could make ideas go viral, you could tip elections, start movements, revolutionise industries.” He told me that Spartz Inc specialised in “fun stuff – entertainment, not hard news”. He called it a media company, but it sounded more like an aggregator and distributor of pre-existing content. “The ability to spread a meme to millions of people,” he continued, was “the closest you can come to a human superpower.” As far as I could tell, Emerson Spartz wasn’t using his memetic superpower either for good or for evil, exactly. He was using it mainly to monetise cat gifs. He told me that his company oversaw about 30 active sites, each serving up procrastination fodder for adolescents of all ages: Memestache (“All the Funny Memes”), OMGFacts (“The World’s #1 Fact Source”), GivesMeHope (“Chicken Soup for the Soul – the 21st-century, Twitter-style version”). The content was mostly user-generated and unvetted, and it just kept rolling in. “Even though I’m one of the most avid readers I know, I don’t usually read straight news,” he told me. “It’s conveyed in a very boring way, and you tend to see the same patterns repeated again and again.” Still, he was happy to offer advice. Glancing down at my laminated badge for the first time, Spartz noticed that I worked at the New Yorker. “For instance, here’s how I would improve your product,” he said. “Way more images. That’s number one. Who has ever looked at a big long block of text and gone, ‘Ooh, exciting?’ I tell my employees all the time: Every paragraph they write should be super-short, no more than three sentences. And I mean short sentences. Periods are better than commas. Boredom is the enemy.” I couldn’t deny that this sounded like an effective recipe for a certain kind of success. And yet, I sputtered, if maximising clicks was the only goal, why would any magazine or newspaper need to employ fact-checkers – or reporters, for that matter? Why not simply recycle press releases, rewriting the boring quotes to make them snappier? Why not replace all Syria coverage with Kardashian coverage? Why not forget about words altogether and go into something more remunerative, like video, or mobile gaming, or strip mining? Spartz cocked his head and waited for me to finish my rant. Clearly, in his eyes, I was revealing myself to be a luddite. “It’s always possible to make a slippery-slope argument,” he said. “Those arguments don’t interest me. I’m interested in impact.” Art without an audience was mere solipsism, he said. “The ultimate barometer of quality is: if it gets shared, it’s quality. If someone wants to toil in obscurity, if that makes them happy, that’s fine. Not everybody has to change the world.” Spartz, in his speeches, sometimes referred to himself as a “growth hacker”. In practice, though, he was more like a day trader, investing in memes that appeared to have momentum. “Exactly where we find our source material took a lot of experimentation to get right,” he said. “But the core of it is simple: taking stuff that’s already going viral and repackaging it.” His proprietary algorithm scoured the internet for images and stories that seemed to be generating a lot of activating emotion (at least, according to the relevant metrics). The content producers then acted as arbitrageurs, adapting those images and stories into lists on Dose, his flagship site. Sometimes this required a bit of reassembly; other times, it was as simple as copying the source material in full, without bothering to rearrange any images or correct any typos, and then reposting it on Dose under a catchier headline. In 2014, there were governmental regulations, imperfect though they may have been, preventing pharmaceutical companies from filling their gelcaps with sawdust, or public-school teachers from filling their lesson plans with Holocaust denialism. Media was different. For many good reasons, starting with the first amendment, the information market was relatively unregulated. And yet everyone knew the bromides, no less true for being trite, about how a democracy can’t function without a well-informed electorate. In the near future, what was to prevent large swaths of the internet – including the parts of the internet that used to be called newspapers and magazines – from looking more and more like Dose? What was insulating the American press from a full-speed race to the bottom? Nothing, as far as I could tell, other than tradition and inertia and the capricious whims of the market. Spartz was proud to make a living on the internet, he said, because it was the closest humanity had yet come to creating a pure meritocracy. “At the 30,000-foot level, the internet is a giant machine that gives people what they want,” Spartz said. “How can you do better than that? It exposes people to the best stuff in the world.” I made the obvious rejoinder: it also exposes people to the worst stuff in the world. “Well, that would be your subjective judgment,” he said, pique rising in his voice. “That’s you paternalistically deciding what’s bad for people. Besides, businesses exist to serve the market. You can have whatever personal values you want, but businesses that don’t provide what the customers want don’t remain businesses. Literally, never.” Once, Spartz told me, “The future of media is an ever-increasing degree of personalisation. My CNN won’t look like your CNN. So we want Dose eventually to be tailored to each user.” On a whiteboard behind him were the phrases “old media”, “Tribune” and “$100 M”. He continued: “You shouldn’t have to choose what you want, because we will be able to get enough data to know what you want better than you do.” In Liar’s Poker, his 1989 Wall Street memoir, Michael Lewis described a newly ascendant, egregiously conceited type of alpha-male bond broker. This type had a name: they called each other Big Swinging Dicks. “Everyone wanted to be a Big Swinging Dick,” Lewis wrote, “even the women.” A quarter of a century later, the A-list entrepreneurs of Silicon Valley occupied an analogous place in the American power structure, but their self-presentation was less aggressive. Instead of “Greed is good”, their aspirational bromides were “Think different” and “Don’t be evil”. Instead of Dionysian feats of consumption – Porsches and cocaine binges and morning cheeseburgers – they drove electric cars and subsisted on seaweed and Soylent. They didn’t deny themselves the pleasures of good old-fashioned capital, but they were equally covetous of social and intellectual capital. Their fondest wish was to be considered luminaries, Renaissance men, the smartest guys in the room. They were Big Swinging Brains. “There is much to discover on the Facebook, the online community for college students,” a Washington Post reporter wrote in the paper’s Style section in late 2004. She did warn, however, that “it’s all a little fake – the ‘friends’; the profiles that can be tailored to what others find appealing; the ‘groups’ that exist only in cyberspace.” A few weeks later, Mark Zuckerberg, looking for investors, visited the office of the Washington Post and met with Donald Graham, the paper’s publisher and CEO. They agreed on a verbal deal: the Post would pay $6m for 10% of the company. Zuckerberg later called Graham in tears – a Silicon Valley venture-capital firm had offered a more generous investment, and he was tempted to take it. Graham, impressed by the young man’s display of rectitude, gave him his blessing to renege on the deal. Three years later, Graham joined Facebook’s board of directors. “Facebook has completely transformed how people interact,” he said in a press release. “Mark’s sense of what Facebook can do is quite remarkable.” In 2007, a Washington Post columnist lamented the rapid ascent of “Amazon.com”, which was “so smart in the way they cater to human weakness, bad judgment, poor taste”. In 2008, another Washington Post columnist wrote: “I loathe Amazon even though I know it is the future and will prevail.” In 2013, with revenue in decline, Donald Graham sold the Washington Post, which his family had owned and overseen for 80 years, to Jeff Bezos, the founder and CEO of Amazon, soon to be the richest person in the world. By that time, it no longer made sense to think of business and tech and media as separate entities. Business was tech, and tech was taking over everything: movies, TV, travel, journalism. Whether the nerd princelings of Silicon Valley understood themselves to be gatekeepers or not, it was becoming increasingly clear that their smallest impromptu decisions were having enormous downstream effects on how billions of people spoke and thought and, ultimately, acted in the world. To change how we talk is to change who we are. I wondered whether they found this power burdensome, and if so, whether they found the burden humbling, or overwhelming – the way I would feel over-whelmed if I woke up to discover that I had somehow been put in charge of the energy grid, or some other key piece of infrastructure that I didn’t fully understand. Maybe Big Swinging Brains were constitutionally incapable of feeling overwhelmed. In any case, there was no law that said you had to understand a piece of social infrastructure in order to own it, or to break it. Business was tech and tech was media. Content was content was content, and coders controlled the sluices through which all content flowed. The luminaries of Silicon Valley didn’t hesitate to offer their bold opinions on almost every subject; and yet, when it came to basic questions about the future of media, their rhetoric turned fuzzy. Businesses should give customers what they want. Media companies should meet audiences where they are. Journalism should be objective and thorough. These truisms seemed unobjectionable enough until they came into conflict with one another, which happened all the time. What if your customers claimed to want rigorous, dispassionate journalism, but their browsing habits revealed that they actually wanted hot takes and salacious hate-reads? What if, in order to meet customers where they were, you had to bowdlerise your writing, or give up on writing altogether and pivot to video? What if quality and popularity were sometimes correlated negatively, or not at all? In early 2016, I was invited to a lunch discussion in an executive boardroom. At the head of the table, a Big Swinging Brain – one of the Biggest – talked for more than an hour without touching his sandwich. He dilated on a wide array of topics (state healthcare exchanges, the future of the trucking industry, the financial panic of 1873), displaying uncanny recall and mental acuity. He acknowledged dilemmas and contradictions in his thinking; he even pointed out awkward conflicts between what he found preferable economically and what might be preferable civically, even morally. I began to wonder whether I’d underestimated the BSBs. Maybe I should learn to stop worrying and love my overlords. Then I asked him a question about the importance of good journalism and good art, the corrosive effects of bad journalism and bad art, and the best way to forestall the Spartzification of the internet. It seemed clear – not just to me, but to anyone who was paying attention – that things were drifting in an unnerving direction. How would humanity avoid a clickbait death spiral? “I don’t think there’s an answer to that,” he said, his tone suddenly turning flinty. Apparently I had revealed myself to be a luddite. “If I were in the media business, I would focus on making a product that people actually want. Because that’s how business works.” I couldn’t imagine him being so flippantly fatalistic about any other civilisational hazard that the free market had failed to address. The Renaissance men of Silicon Valley were known for spending an unusual amount of time and money addressing thorny problems, such as the achievement gap in American public schools and the excess of carbon in the atmosphere. They even invested millions of dollars in problems that hadn’t come into existence yet, such as hostile AI. In 2016, the Chan Zuckerberg Initiative, the nonprofit founded by Mark Zuckerberg and his wife, Priscilla Chan, announced its intention to “help cure, prevent, and manage all disease in our children’s lifetime”; several well-capitalised bioengineering start-ups, including a $1.5bn initiative at Google, went even further, resolving to cure death. But somehow the BSBs balked at the problem of addictive, low-quality clickbait. They had taken control of the media industry, then moved fast and broken it; now they claimed no responsibility for fixing it. The techno-utopians of Silicon Valley assumed that all would be for the best in a post-gatekeeper world. This was possible, of course, but there was no way to be certain. Already, social media-optimised content mills were outcompeting sober policy journals and threadbare alt-weeklies. Pulitzer prize-winning reporters, unable to earn a living wage, kept fleeing journalism for jobs in PR or social media marketing. Even an alarmist like myself didn’t presume that the Spartzification of the entire media ecosystem would happen overnight. Could it happen within five years? Fifteen? I tried telling myself that I was indulging in slippery-slope thinking, but this did nothing to allay my fear that we were already slipping. This is an edited extract from Antisocial: How Online Extremists Broke America, by Andrew Marantz, published by Picador on 20 Feb and available at guardianbookshop.co.uk • Follow the Long Read on Twitter at @gdnlongread, and sign up to the long read weekly email here.
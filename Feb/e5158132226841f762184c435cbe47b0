New regulation should be passed to control the algorithms that promote content such as posts, videos and adverts on social networks, the UK government’s advisory body on AI ethics has recommended. As part of a forthcoming overhaul of regulation covering the internet, the government should also consider requiring social media platforms to allow independent researchers access to their data if they are researching issues of public concern, the Centre for Data Ethics and Innovation (CDEI) suggested. That could include topics such as the effects of social media on mental health, or its role in spreading misinformation. New regulations should also require the creation of publicly accessible online archives for “high-risk” adverts, mirroring those voluntarily created by the social networks for political adverts, but expanding their remit to cover areas such as jobs, housing, credit and age-restricted products, the CDEI said. Roger Taylor, the centre’s chair, said: “Most people do not want targeting stopped. But they do want to know that it is being done safely and responsibly. And they want more control. Tech platforms’ ability to decide what information people see puts them in a position of real power. To build public trust over the long term it is vital for the government to ensure that the new online harms regulator looks at how platforms recommend content, establishing robust processes to protect vulnerable people.” The report comes as the government plans how best to legislate the goals laid out in last year’s online harms white paper, which suggested a number of aims for a new internet regulator, including suppressing the spread of legal-but-harmful content such as material that encourages self-harm or eating disorders. The CDEI proposed that the same legislation could be expanded to create a regulator that ensured other elements of the web were no longer “out of step with the public’s expectations”. The centre, which was launched by the then chancellor, Philip Hammond, in 2017, cited polling suggesting that fewer than a third of Britons “trust platforms to target them in a responsible way”, and that almost two-thirds, 61%, “favoured greater regulatory oversight of online targeting”. Just 17% of people polled supported the current system of self-regulation for online targeting. Some of the proposals are likely to spark pushback, however. Greater access for academics to social media data could help answer difficult questions about the effects of new technologies on societal problems, but it could also lead to fresh sources of data breaches or privacy violations: the primary source of Facebook data used by the infamous election consultancy Cambridge Analytica, for instance, was a psychology academic from the University of Cambridge, who passed the data on against Facebook’s terms of service. The CDEI’s report comes as a second study, released by the media regulator Ofcom, shows that parents increasingly feel that the risks of their children being online outweighs the benefits. While a majority of parents still think the internet is a net good, the proportion has dropped from 65% five years ago to 55% now, with about 2 million parents arguing that the benefits are outweighed by the risks. Parents questioned by Ofcom were specially concerned by the risks of their child seeing content that might encourage them to harm themselves, by the social and commercial pressure to buy in-game items such as loot boxes while playing online, and by online bullying via video games. But the regulator also found an increase in online social activism among children, calling the phenomenon the “Greta effect”, as almost one in five 12- to 15-year-olds reported using social media to express support for political, environmental or charitable causes or organisations.